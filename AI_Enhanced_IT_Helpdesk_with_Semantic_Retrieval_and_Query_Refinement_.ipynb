{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68Ix-puKjLjh",
    "outputId": "e1872e2b-5245-43ba-8fde-9f83b3e81781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting groq\n",
      "  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading groq-0.24.0-py3-none-any.whl (127 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, groq\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed faiss-cpu-1.11.0 groq-0.24.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.59)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.42)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
      "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: docx2txt, python-docx\n",
      "Successfully installed docx2txt-0.9 python-docx-1.1.2\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy sentence-transformers langchain faiss-cpu groq openai\n",
    "!pip install langchain-community\n",
    "!pip install docx2txt python-docx\n",
    "!pip install --upgrade langchain\n",
    "\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "819001e946224b628148ed0a2bed05f5",
      "08f035f495e84cba94ed2202a50d0938",
      "9f8a60ab86df40ee82c45449666feada",
      "8ec843cf63e74020b839c71bea3139e2",
      "7f8ebec6eb154168a02d8f7e36db1c12",
      "fbecddc0e3e6446fa0ca2d222d082d31",
      "64561b57f78543f1a9346697a723123e",
      "6f637487883a4f9fb5c94386d8c945cb",
      "ddc68991d3e44039afcfd5e32fdaee0b",
      "b862b1b36149419695fe09fa66e3366e",
      "70d1b766044a4f66afe31bcabd58c866",
      "245014328f32404ea22592852ae99f9b",
      "ccf227994ebf424dbcc6cf2c1367597e",
      "9a9738753ba44c3ba7f9faa221994976",
      "17da32159192476d81b2042ef5274dfb",
      "ddd14a644be045c2accba11ea5dec605",
      "bfe44447701941f38285326c3d34a402",
      "7f6257a46cb3484aa43ec53e21ca5894",
      "f9c03a7ce4724ee8b2e8060ed1bba576",
      "ba92dbf270c84909ac5b8a1818d49afe",
      "94249c18050b40bca80a408599b2b78c",
      "e3b7ad3bf90f438b8648cc4f4d997bc8",
      "844b531b54af49b483fc83909af3af1e",
      "14cafa472ff94ae4a4d649de8fddf2f5",
      "c2bccd2810c04be49e758e99cecc850d",
      "4c743491af8d41f59314a04eac5aab8e",
      "402d9c853b844b8f81569c6421d2a071",
      "deabe6eef77b4bf6878aed89fb104c40",
      "9774a5edd71a4e57b592004834aebae4",
      "974fc407f5db45eb842bec0c910b242b",
      "d903b388c8864b27adcb5da48a5c2435",
      "2f9845d424fa4faa9300267435645ffc",
      "424483653e104ce7ad85a8f04d83e254",
      "39af8a652cf648aebe129f2c3807642c",
      "bbb81207bac947ab809012f057b1e523",
      "02d62216c48b46089276044cd0573865",
      "432b634ca0be44d4926e0d03760ccb0c",
      "7121bf6c5d694aeca76c8bed572df99f",
      "b4e67faa4e8c4102b876c8451d9d83d3",
      "ba8e832c8fbd4714bae3bf19d3b0f586",
      "1c0dae49865544509bc1b0b775d62162",
      "c6918c1ff8f44079bfa1b832abf3da86",
      "bbae5c5bdb3745bca61a9f9e408679aa",
      "239dcb2d2f0546b98ac2ff9f099614f6",
      "2af56c117f534cc7a892bec670463898",
      "a52baefb04044603a11113b95ec4b1f1",
      "e968222dab834065842efae82716918a",
      "3d4dc7011f5f402ea132d9d0b7aacc24",
      "ba2818dfc3574335b8e9c938583c8a69",
      "3312b65855994be0a54d970e008da088",
      "00c21b1cd8ba4cfd8b0dc21377da98e3",
      "2f6b1871aa244f72a1084cb495a10efe",
      "91a82bbcb4fb4419a1598442db5f9282",
      "fda2189f32a640faaa8a7868483532a9",
      "dc953bff598b44019e87d1ef0e339909",
      "9a5c035e2af141c5b2a7f5884cdb4cff",
      "90bf3fe74ef64621ae4284eb3939a310",
      "e9d7b0a3b11f45978af2a1572a2b942c",
      "8e2c24187eee4beb985aed560d56abc4",
      "17f46a8eae614683bf6895920df72aeb",
      "82dd95ea5a3e487cbdd14748ef872b52",
      "0f3c619fb20e4c70a7baf080ed46f300",
      "431bbc956f6e419e9539e719330e3091",
      "178f8b4d134a47ebb7ff50e71661874a",
      "c7161d11cbc8455b8a124f18661edc2b",
      "f15438c3a4484217ba589c000eb3443c",
      "d46d23dae88f410d84af5e60f0bbb72b",
      "2ec776464b9c41d09d51ead4190aa78e",
      "c28794a3cec34fb585f6ce98a5961f99",
      "9998ee55b24545ff8591a68c5dfefae0",
      "807ce65715ce4838ab6b6362ab69c84f",
      "2ff9fb96d9624e84ae7591e26f33c54a",
      "5b988fb408ef478cba90bec1e4b0b39c",
      "ef381e1cf68a465a8ef3786f0a32fcae",
      "061e5c4cdb304b4b8867667d0ba1d1cd",
      "1150be7c4cb9468ea9e3fe2289deae90",
      "78b956af62b3406ea24e35df0cddbcc4",
      "cfb5b0db852c4b62a0bc9efb2410d550",
      "5acb36f4df8940d6b0918836b63cc138",
      "b55b6682408b4586956e76ff7a209432",
      "f8fbebd9a29c40c4a6b6b34b12b111a6",
      "fe284a04b09242348129b73d88507fd1",
      "77c3a3b6155d4512b4db011aa0092363",
      "48ad8d3f036b493e9ed6af1286343df2",
      "5dda022ac4c84ec0b7fd2b710d0ce56e",
      "b381318f12b3487c82ce12ba03bc0340",
      "a03050c43c5b4b25804f6d2239cffd3a",
      "8adb6ad92c1b42fb94cde7f3beb9aed8",
      "dc4e75b98cbb42cb817739e50513828f",
      "89aabaaf2509497286bb3d997296cdc3",
      "fa43d518b296429a84b7b5583a9b3d99",
      "b0db46b84e52402db7f1b21edb467d97",
      "79dcb56713d44d91978976115dd1ffcc",
      "1db5f0386a5d4f4dafa85056e8618c3f",
      "76e98dd505e349f0b6a9d65b4d8d5e7b",
      "407b641b6b3b41188bd66f15c12de4f3",
      "34661a18f6f045e3a77246ecc35add85",
      "9e343d479cf442abb0433c993b3c82f9",
      "218327f5aba34aafae7ef92d0b01e141",
      "bc3492433b894211941e61d7bd4bf31c",
      "9b186fa6dc8f461f9c85da63b387a9ba",
      "0b3bdd4fc7e34a58bb3bbb46abe86697",
      "2e5a138cf9ef44e3a960c5e101f3bbb9",
      "73143e59c6794c4c87048dc3dc0e4cb1",
      "4043c3efcbf545ae85fe7ce604646aed",
      "889f19fb36234a15bccd9a1c0652267f",
      "1997c413cced4f06aac9b2a4af635e3a",
      "3d3e0c1d78174305bffbab5eb3bae3ae",
      "9a6d039881fe4da79af9c4bb76b399aa",
      "70030d5359ee4ca89b398ede891ea088",
      "8f8f3ad231144d578498b2ddf75b95e0",
      "4f22734092fa464cb1e11f2446660bbc",
      "f22bbefd67cf4dacaba15739df4badb0",
      "ba1c0e78f793422ca031500b72d4cfb0",
      "d49df218ec594353aed16a8562054a3e",
      "9e811976992b4b1e9a2cdd81957dd0f1",
      "d823d90eb6d14b3493f7bb5b74d56c3c",
      "b0ac89b1b71840c9b6b838f1c70d8aaf",
      "78354ae385964084a4ae8e3955e4084c",
      "0eef2e8425e14850abed5180cfec8608",
      "7e2d4e8674e04a6e8670c40d844e0219"
     ]
    },
    "id": "7P-YY5_9-SN3",
    "outputId": "c095b88b-a708-434d-da09-b70b589b406f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /content\n",
      "Extracting data from: /content/Dataset of all topic.docx\n",
      "Extracted 3 entries from the document\n",
      "Data loaded successfully.\n",
      "Number of rows: 3\n",
      "Number of unique topics: 3\n",
      "First few rows:\n",
      "   S.No            Topic                                        Description\n",
      "0  None  1.LOAD BALANCER  A load balancer is a device or software that d...\n",
      "1  None        2.ROUTERS  A router is a networking device that forwards ...\n",
      "2  None       3.FIREWALL  A firewall is a network security device or sof...\n",
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819001e946224b628148ed0a2bed05f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245014328f32404ea22592852ae99f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844b531b54af49b483fc83909af3af1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39af8a652cf648aebe129f2c3807642c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af56c117f534cc7a892bec670463898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5c035e2af141c5b2a7f5884cdb4cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46d23dae88f410d84af5e60f0bbb72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb5b0db852c4b62a0bc9efb2410d550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4e75b98cbb42cb817739e50513828f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3492433b894211941e61d7bd4bf31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8f3ad231144d578498b2ddf75b95e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-afefea91bc2d>:75: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrameLoader...\n",
      "Splitting documents semantically...\n",
      "Loading and splitting documents...\n",
      "Total documents after splitting: 7\n",
      "Creating FAISS vector store...\n",
      "Vector store created successfully\n",
      "Enter your query about computer networking or IT problems: how to hack my router \n",
      "Processing query...\n",
      "Refining query...\n",
      "Query refined: I cannot provide information or guidance on illegal or harmful activities. Instead, I suggest refining your query to focus on legal and ethical ways to manage and secure your router. Here's a refined version of your query:\n",
      "\n",
      "\"How to secure my router and improve its settings for optimal performance and security.\"\n",
      "or\n",
      "\"How to configure my router's settings for better Wi-Fi coverage and password protection.\"\n",
      "or\n",
      "\"How to troubleshoot common router issues and optimize its performance.\"\n",
      "Refined query: I cannot provide information or guidance on illegal or harmful activities. Instead, I suggest refining your query to focus on legal and ethical ways to manage and secure your router. Here's a refined version of your query:\n",
      "\n",
      "\"How to secure my router and improve its settings for optimal performance and security.\"\n",
      "or\n",
      "\"How to configure my router's settings for better Wi-Fi coverage and password protection.\"\n",
      "or\n",
      "\"How to troubleshoot common router issues and optimize its performance.\"\n",
      "Searching for relevant information...\n",
      "Preparing context for answer generation...\n",
      "Document 1:\n",
      "  Topic: 3.FIREWALL\n",
      "  Description: Refers to the process of creating, setting up, and executing a network infrastructure in networking ...\n",
      "  Original Index: None\n",
      "Document 2:\n",
      "  Topic: 2.ROUTERS\n",
      "  Description: A router is a networking device that forwards data packets between computer networks. It directs tra...\n",
      "  Original Index: None\n",
      "Document 3:\n",
      "  Topic: 3.FIREWALL\n",
      "  Description: to permit or prohibit traffic based on IP addresses, ports, protocols, and other characteristics, AC...\n",
      "  Original Index: None\n",
      "Generating final answer...\n",
      "Generating answer...\n",
      "Answer generated successfully\n",
      "Final Answer:\n",
      "Based on the retrieved context, I'll provide a detailed answer to the refined query.\n",
      "\n",
      "Query: How to secure my router and improve its settings for optimal performance and security.\n",
      "\n",
      "Description: Securing a router involves configuring its settings to prevent unauthorized access, monitor network traffic, and restrict access to specific areas of the network. Improving router settings can enhance its performance by optimizing Wi-Fi coverage, reducing latency, and increasing data transfer rates.\n",
      "\n",
      "Original Index: None\n",
      "S.No: None\n",
      "\n",
      "Problem: Unsecured routers are vulnerable to hacking, eavesdropping, and malware attacks, compromising network security and data confidentiality.\n",
      "\n",
      "Cause: Poor configuration, outdated firmware, weak passwords, and lack of Firewall and Access Control Lists (ACLs) setups.\n",
      "\n",
      "Solution:\n",
      "\n",
      "1. Change the default administrator password and username to strong, unique passwords.\n",
      "2. Ensure the router's firmware is up-to-date and has the latest security patches.\n",
      "3. Set up a Firewall to block incoming and outgoing traffic based on IP addresses, ports, and protocols.\n",
      "4. Configure Access Control Lists (ACLs) to permit or prohibit traffic based on IP addresses, ports, protocols, and other characteristics.\n",
      "5. Use Network Address Translation (NAT) to conceal internal IP addresses and reduce the risk of IP address takeover.\n",
      "6. restrict access to specific areas of the network by creating Virtual Local Area Networks (VLANs).\n",
      "7. Monitor network traffic and detect suspicious activity using network traffic analysis software.\n",
      "8. Configure Quality of Service (QoS) settings to prioritize critical network traffic, reducing latency and increasing performance.\n",
      "\n",
      "Type: Router Configuration and Security\n",
      "\n",
      "OSI Layer: Transport Layer (Layer 4), Network Layer (Layer 3), and Data Link Layer (Layer 2)\n",
      "\n",
      "Algorithm:\n",
      "\n",
      "1. Packet Filter Algorithm: used to filter incoming and outgoing packets based on source and destination IP addresses, ports, and protocols.\n",
      "2. NAT Algorithm: used to replace internal IP addresses with a public IP address when forwarding packets.\n",
      "3. VLAN Algorithm: used to create logical networks within the same physical network infrastructure.\n",
      "\n",
      "Detailed Solution:\n",
      "\n",
      "1. Log in to the router's web interface and change the administrator password to a strong, unique password.\n",
      "2. Check for firmware updates and install the latest version.\n",
      "3. Configure the Firewall to block incoming and outgoing traffic based on IP addresses, ports, and protocols. Apply the Firewall settings to the WAN interface.\n",
      "4. Create an ACL to permit or prohibit traffic based on IP addresses, ports, protocols, and other characteristics. Apply the ACL to the LAN interface.\n",
      "5. Enable NAT to conceal internal IP addresses and reduce the risk of IP address takeover.\n",
      "6. Create a VLAN to restrict access to specific areas of the network. Configure the VLAN settings on the router and on client devices.\n",
      "7. Monitor network traffic using network traffic analysis software, such as Wireshark or CiscoWorks.\n",
      "8. Configure QoS settings to prioritize critical network traffic, such as voice and video traffic.\n",
      "\n",
      "By following these steps, you can secure your router and improve its settings for optimal performance and security.\n",
      "\n",
      "Sources:\n",
      "Document 1 - Topic: 3.FIREWALL, Original Index: None\n",
      "Document 2 - Topic: 2.ROUTERS, Original Index: None\n",
      "Document 3 - Topic: 3.FIREWALL, Original Index: None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from groq import Groq\n",
    "import docx2txt\n",
    "from docx import Document\n",
    "import io\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Function to extract text and Excel data from a single Word document\n",
    "def extract_data_from_doc(file_path):\n",
    "    print(f\"Extracting data from: {file_path}\")\n",
    "    # Extract text content\n",
    "    text_content = docx2txt.process(file_path)\n",
    "\n",
    "    # Parse text content to extract topics, descriptions, and original index\n",
    "    text_data = []\n",
    "    lines = text_content.split('\\n')\n",
    "    current_topic = \"\"\n",
    "    current_description = \"\"\n",
    "    current_index = None\n",
    "    for line in lines:\n",
    "        if line.strip().isdigit():  # Assuming the original index is a number (S.No)\n",
    "            current_index = line.strip()\n",
    "        elif line.strip().isupper():  # Assuming topics are in uppercase\n",
    "            if current_topic and current_description:\n",
    "                text_data.append({\"S.No\": current_index, \"Topic\": current_topic, \"Description\": current_description.strip()})\n",
    "            current_topic = line.strip()\n",
    "            current_description = \"\"\n",
    "        else:\n",
    "            current_description += line + \" \"\n",
    "    if current_topic and current_description:\n",
    "        text_data.append({\"S.No\": current_index, \"Topic\": current_topic, \"Description\": current_description.strip()})\n",
    "\n",
    "    # Extract Excel data (if any)\n",
    "    doc = Document(file_path)\n",
    "    excel_data = []\n",
    "    for table in doc.tables:\n",
    "        headers = [cell.text for cell in table.rows[0].cells]\n",
    "        for row in table.rows[1:]:\n",
    "            row_data = {headers[i]: cell.text for i, cell in enumerate(row.cells)}\n",
    "            excel_data.append(row_data)\n",
    "\n",
    "    # Combine text and Excel data\n",
    "    combined_data = text_data + excel_data\n",
    "    print(f\"Extracted {len(combined_data)} entries from the document\")\n",
    "    return pd.DataFrame(combined_data)\n",
    "\n",
    "# Load data from the document\n",
    "file_path = '/content/Dataset of all topic.docx'  # Correct file path to the uploaded file\n",
    "try:\n",
    "    combined_df = extract_data_from_doc(file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Number of rows: {len(combined_df)}\")\n",
    "    print(f\"Number of unique topics: {combined_df['Topic'].nunique()}\")\n",
    "    print(\"First few rows:\")\n",
    "    print(combined_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    print(\"Please make sure the file is in the correct location and you have the necessary permissions.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a DataFrameLoader\n",
    "print(\"Creating DataFrameLoader...\")\n",
    "loader = DataFrameLoader(combined_df, page_content_column='Description')\n",
    "\n",
    "# Semantic Chunking - Split documents by paragraphs or larger semantic units\n",
    "print(\"Splitting documents semantically...\")\n",
    "\n",
    "# Using RecursiveCharacterTextSplitter to create semantic chunks (with overlap)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunk size\n",
    "    chunk_overlap=100,  # Overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"],  # Splitting first by paragraphs, then by lines, then by spaces\n",
    ")\n",
    "\n",
    "# Load documents and split them using the semantic chunking approach\n",
    "print(\"Loading and splitting documents...\")\n",
    "documents = loader.load()\n",
    "documents = splitter.split_documents(documents)\n",
    "\n",
    "# Add the metadata, ensuring the original index (S.No) is included\n",
    "j = 0  # Initialize a separate counter for dataframe\n",
    "for i, doc in enumerate(documents):\n",
    "    if 'S.No' in combined_df.columns and j < len(combined_df):\n",
    "        doc.metadata['S.No'] = combined_df.iloc[j]['S.No']\n",
    "        j += 1  # Increment the counter for the dataframe\n",
    "\n",
    "print(f\"Total documents after splitting: {len(documents)}\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "print(\"Creating FAISS vector store...\")\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "print(\"Vector store created successfully\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    print(\"Refining query...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries about computer networking topics and IT problems to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following query about computer networking or IT problems for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        refined_query = chat_completion.choices[0].message.content.strip()\n",
    "        print(f\"Query refined: {refined_query}\")\n",
    "        return refined_query\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context, sources):\n",
    "    print(\"Generating answer...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant specializing in computer networking and IT problem-solving. Generate detailed answers to user queries based on the retrieved context. Include information about the OSI layer, algorithms involved, and a detailed solution if applicable.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context about computer networking topics and IT problems, provide a detailed answer to the query. Include the description, original index, problem, cause, solution, type, OSI layer, and algorithm if available. Then generate a detailed solution:\\n\\nContext: {retrieval_context}\\n\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content.strip()\n",
    "        print(\"Answer generated successfully\")\n",
    "\n",
    "        # Append sources to the answer\n",
    "        sourced_answer = f\"{answer}\\n\\nSources:\\n\" + \"\\n\".join(sources)\n",
    "        return sourced_answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    query_text = input(\"Enter your query about computer networking or IT problems: \")\n",
    "\n",
    "    print(\"Processing query...\")\n",
    "    refined_query_text = refine_query_with_groq(query_text)\n",
    "    print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "    print(\"Searching for relevant information...\")\n",
    "    retrieved_documents = vector_store.similarity_search(refined_query_text, k=3)\n",
    "\n",
    "    print(\"Preparing context for answer generation...\")\n",
    "    retrieval_context = \"\"\n",
    "    sources = []  # List to hold sources for output\n",
    "    for i, doc in enumerate(retrieved_documents):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(f\"  Topic: {doc.metadata.get('Topic', 'Unknown')}\")\n",
    "        print(f\"  Description: {doc.page_content[:100]}...\")  # Print first 100 chars\n",
    "        print(f\"  Original Index: {doc.metadata.get('S.No', 'Unknown')}\")  # Ensure original index is printed\n",
    "        retrieval_context += f\"Topic: {doc.metadata.get('Topic', 'Unknown')}\\n\"\n",
    "        retrieval_context += f\"Description: {doc.page_content}\\n\"\n",
    "        retrieval_context += f\"Original Index: {doc.metadata.get('S.No', 'Unknown')}\\n\"\n",
    "        for key, value in doc.metadata.items():\n",
    "            if key != 'Topic':\n",
    "                retrieval_context += f\"{key}: {value}\\n\"\n",
    "        retrieval_context += \"\\n\"\n",
    "        # Add document source information to sources list\n",
    "        sources.append(f\"Document {i+1} - Topic: {doc.metadata.get('Topic', 'Unknown')}, Original Index: {doc.metadata.get('S.No', 'Unknown')}\")\n",
    "\n",
    "    print(\"Generating final answer...\")\n",
    "    final_answer = generate_answer_with_rag(refined_query_text, retrieval_context, sources)\n",
    "    print(f\"Final Answer:\\n{final_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkN9OY7Ly5Sh",
    "outputId": "0d1c9b1a-7859-4539-e6de-4b9fa2bab2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /content\n",
      "Extracting data from: /content/Dataset of all topic.docx\n",
      "Extracted 3 entries from the document\n",
      "Data loaded successfully.\n",
      "Number of rows: 3\n",
      "Number of unique topics: 3\n",
      "First few rows:\n",
      "   S.No            Topic                                        Description\n",
      "0  None  1.LOAD BALANCER  A load balancer is a device or software that d...\n",
      "1  None        2.ROUTERS  A router is a networking device that forwards ...\n",
      "2  None       3.FIREWALL  A firewall is a network security device or sof...\n",
      "Loading embedding model...\n",
      "Creating DataFrameLoader...\n",
      "Splitting documents semantically...\n",
      "Loading and splitting documents...\n",
      "Total documents after splitting: 7\n",
      "Creating FAISS vector store...\n",
      "Vector store created successfully\n",
      "Enter your query about computer networking or IT problems: how to hack my router \n",
      "Processing query...\n",
      "Refining query...\n",
      "Query refined: I cannot provide information or guidance on illegal or harmful activities, including hacking. \n",
      "\n",
      "Is there anything else I can help you with?\n",
      "Refined query: I cannot provide information or guidance on illegal or harmful activities, including hacking. \n",
      "\n",
      "Is there anything else I can help you with?\n",
      "Searching for relevant information...\n",
      "Preparing context for answer generation...\n",
      "Document 1:\n",
      "  Topic: 3.FIREWALL\n",
      "  Description: Refers to the process of creating, setting up, and executing a network infrastructure in networking ...\n",
      "  Original Index: None\n",
      "Document 2:\n",
      "  Topic: 3.FIREWALL\n",
      "  Description: A firewall is a network security device or software that monitors and controls incoming and outgoing...\n",
      "  Original Index: None\n",
      "Document 3:\n",
      "  Topic: 3.FIREWALL\n",
      "  Description: Involves configuring and overseeing wireless communication between networked devices. It covers a nu...\n",
      "  Original Index: None\n",
      "Generating final answer...\n",
      "Generating answer...\n",
      "Answer generated successfully\n",
      "Final Answer:\n",
      "I cannot provide information or guidance on illegal activities, including hacking.\n",
      "\n",
      "Sources:\n",
      "- LLM Model: Llama 3 (8B parameters)\n",
      "- Websites:\n",
      "  1. [Networking Expertise](https://www.networkingexpertise.com/search?q=I%2Bcannot%2Bprovide%2Binformation%2Bor%2Bguidance%2Bon%2Billegal%2Bor%2Bharmful%2Bactivities%2Bincluding%2Bhacking%2B%0D%0A%0D%0AIs%2Bthere%2Banything%2Belse%2BI%2Bcan%2Bhelp%2Byou%2Bwith)\n",
      "  2. [Pearson Sample](https://ptgmedia.pearsoncmg.com/images/9780789759818/samplepages/9780789759818_Sample.pdf#search=I%2Bcannot%2Bprovide%2Binformation%2Bor%2Bguidance%2Bon%2Billegal%2Bor%2Bharmful%2Bactivities%2Bincluding%2Bhacking%2B%0D%0A%0D%0AIs%2Bthere%2Banything%2Belse%2BI%2Bcan%2Bhelp%2Byou%2Bwith)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from groq import Groq\n",
    "import docx2txt\n",
    "from docx import Document\n",
    "import io\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Function to extract text and Excel data from a single Word document\n",
    "def extract_data_from_doc(file_path):\n",
    "    print(f\"Extracting data from: {file_path}\")\n",
    "    # Extract text content\n",
    "    text_content = docx2txt.process(file_path)\n",
    "\n",
    "    # Parse text content to extract topics, descriptions, and original index\n",
    "    text_data = []\n",
    "    lines = text_content.split('\\n')\n",
    "    current_topic = \"\"\n",
    "    current_description = \"\"\n",
    "    current_index = None\n",
    "    for line in lines:\n",
    "        if line.strip().isdigit():  # Assuming the original index is a number (S.No)\n",
    "            current_index = line.strip()\n",
    "        elif line.strip().isupper():  # Assuming topics are in uppercase\n",
    "            if current_topic and current_description:\n",
    "                text_data.append({\"S.No\": current_index, \"Topic\": current_topic, \"Description\": current_description.strip()})\n",
    "            current_topic = line.strip()\n",
    "            current_description = \"\"\n",
    "        else:\n",
    "            current_description += line + \" \"\n",
    "    if current_topic and current_description:\n",
    "        text_data.append({\"S.No\": current_index, \"Topic\": current_topic, \"Description\": current_description.strip()})\n",
    "\n",
    "    # Extract Excel data (if any)\n",
    "    doc = Document(file_path)\n",
    "    excel_data = []\n",
    "    for table in doc.tables:\n",
    "        headers = [cell.text for cell in table.rows[0].cells]\n",
    "        for row in table.rows[1:]:\n",
    "            row_data = {headers[i]: cell.text for i, cell in enumerate(row.cells)}\n",
    "            excel_data.append(row_data)\n",
    "\n",
    "    # Combine text and Excel data\n",
    "    combined_data = text_data + excel_data\n",
    "    print(f\"Extracted {len(combined_data)} entries from the document\")\n",
    "    return pd.DataFrame(combined_data)\n",
    "\n",
    "# Load data from the document\n",
    "file_path = '/content/Dataset of all topic.docx'  # Correct file path to the uploaded file\n",
    "try:\n",
    "    combined_df = extract_data_from_doc(file_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Number of rows: {len(combined_df)}\")\n",
    "    print(f\"Number of unique topics: {combined_df['Topic'].nunique()}\")\n",
    "    print(\"First few rows:\")\n",
    "    print(combined_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    print(\"Please make sure the file is in the correct location and you have the necessary permissions.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a DataFrameLoader\n",
    "print(\"Creating DataFrameLoader...\")\n",
    "loader = DataFrameLoader(combined_df, page_content_column='Description')\n",
    "\n",
    "# Semantic Chunking - Split documents by paragraphs or larger semantic units\n",
    "print(\"Splitting documents semantically...\")\n",
    "\n",
    "# Using RecursiveCharacterTextSplitter to create semantic chunks (with overlap)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunk size\n",
    "    chunk_overlap=100,  # Overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"],  # Splitting first by paragraphs, then by lines, then by spaces\n",
    ")\n",
    "\n",
    "# Load documents and split them using the semantic chunking approach\n",
    "print(\"Loading and splitting documents...\")\n",
    "documents = loader.load()\n",
    "documents = splitter.split_documents(documents)\n",
    "\n",
    "# Add the metadata, ensuring the original index (S.No) is included\n",
    "j = 0  # Initialize a separate counter for dataframe\n",
    "for i, doc in enumerate(documents):\n",
    "    if 'S.No' in combined_df.columns and j < len(combined_df):\n",
    "        doc.metadata['S.No'] = combined_df.iloc[j]['S.No']\n",
    "        j += 1  # Increment the counter for the dataframe\n",
    "\n",
    "print(f\"Total documents after splitting: {len(documents)}\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "print(\"Creating FAISS vector store...\")\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "print(\"Vector store created successfully\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"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\")  # Replace with your actual API key\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    print(\"Refining query...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries about computer networking topics and IT problems to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following query about computer networking or IT problems for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        refined_query = chat_completion.choices[0].message.content.strip()\n",
    "        print(f\"Query refined: {refined_query}\")\n",
    "        return refined_query\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context):\n",
    "    print(\"Generating answer...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant specializing in computer networking and IT problem-solving. Generate detailed answers to user queries based on the retrieved context. Include information about the OSI layer, algorithms involved, and a detailed solution if applicable.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context about computer networking topics and IT problems, provide a detailed answer to the query. Include the description, original index, problem, cause, solution, type, OSI layer, and algorithm if available. Then generate a detailed solution:\\n\\nContext: {retrieval_context}\\n\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content.strip()\n",
    "        print(\"Answer generated successfully\")\n",
    "\n",
    "        # Generate query-specific links\n",
    "        networking_expertise_link = generate_query_link(\"https://www.networkingexpertise.com/search?q=\", refined_query_text)\n",
    "        pearson_link = generate_query_link(\"https://ptgmedia.pearsoncmg.com/images/9780789759818/samplepages/9780789759818_Sample.pdf#search=\", refined_query_text)\n",
    "\n",
    "        # Add modified source attribution with query-specific links\n",
    "        sourced_answer = f\"{answer}\\n\\nSources:\\n- LLM Model: Llama 3 (8B parameters)\\n- Websites:\\n  1. [Networking Expertise]({networking_expertise_link})\\n  2. [Pearson Sample]({pearson_link})\"\n",
    "        return sourced_answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "def generate_query_link(base_url, query):\n",
    "    # Remove special characters and spaces from the query\n",
    "    cleaned_query = re.sub(r'[^\\w\\s]', '', query).replace(' ', '+')\n",
    "    return f\"{base_url}{urllib.parse.quote(cleaned_query)}\"\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    query_text = input(\"Enter your query about computer networking or IT problems: \")\n",
    "\n",
    "    print(\"Processing query...\")\n",
    "    refined_query_text = refine_query_with_groq(query_text)\n",
    "    print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "    print(\"Searching for relevant information...\")\n",
    "    retrieved_documents = vector_store.similarity_search(refined_query_text, k=3)\n",
    "\n",
    "    print(\"Preparing context for answer generation...\")\n",
    "    retrieval_context = \"\"\n",
    "    for i, doc in enumerate(retrieved_documents):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(f\"  Topic: {doc.metadata.get('Topic', 'Unknown')}\")\n",
    "        print(f\"  Description: {doc.page_content[:100]}...\")  # Print first 100 chars\n",
    "        print(f\"  Original Index: {doc.metadata.get('S.No', 'Unknown')}\")  # Ensure original index is printed\n",
    "        retrieval_context += f\"Topic: {doc.metadata.get('Topic', 'Unknown')}\\n\"\n",
    "        retrieval_context += f\"Description: {doc.page_content}\\n\"\n",
    "        retrieval_context += f\"Original Index: {doc.metadata.get('S.No', 'Unknown')}\\n\"\n",
    "        for key, value in doc.metadata.items():\n",
    "            if key != 'Topic':\n",
    "                retrieval_context += f\"{key}: {value}\\n\"\n",
    "        retrieval_context += \"\\n\"\n",
    "\n",
    "    print(\"Generating final answer...\")\n",
    "    final_answer = generate_answer_with_rag(refined_query_text, retrieval_context)\n",
    "    print(f\"Final Answer:\\n{final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKlYO6E8kTVh",
    "outputId": "c655161a-f247-46f9-e0af-adeded8244d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /content\n",
      "Extracting data from: /content/Dataset of all topic.docx\n",
      "Extracted 3 entries from the document\n",
      "Data loaded successfully.\n",
      "Number of rows: 3\n",
      "Number of unique topics: 3\n",
      "First few rows:\n",
      "   S.No            Topic                                        Description\n",
      "0  None  1.LOAD BALANCER  A load balancer is a device or software that d...\n",
      "1  None        2.ROUTERS  A router is a networking device that forwards ...\n",
      "2  None       3.FIREWALL  A firewall is a network security device or sof...\n",
      "Loading embedding model...\n",
      "Creating DataFrameLoader...\n",
      "Splitting documents semantically...\n",
      "Loading and splitting documents...\n",
      "Total documents after splitting: 7\n",
      "Creating FAISS vector store...\n",
      "Vector store created successfully\n",
      "Enter your query about computer networking or IT problems: how to hack my router \n",
      "Processing query...\n",
      "Refining query...\n",
      "Query refined: I cannot assist with illegal activities such as hacking. Instead, I suggest you consider the following refined query: \"How to configure/secure my router?\"\n",
      "\n",
      "This revised query is more specific, and I can help you with that.\n",
      "Refined query: I cannot assist with illegal activities such as hacking. Instead, I suggest you consider the following refined query: \"How to configure/secure my router?\"\n",
      "\n",
      "This revised query is more specific, and I can help you with that.\n",
      "Searching for relevant information...\n",
      "Preparing context for answer generation...\n",
      "Document 1:\n",
      "  Topic: 3.FIREWALL\n",
      "  Description: Refers to the process of creating, setting up, and executing a network infrastructure in networking ...\n",
      "  Original Index: None\n",
      "Document 2:\n",
      "  Topic: 2.ROUTERS\n",
      "  Description: A router is a networking device that forwards data packets between computer networks. It directs tra...\n",
      "  Original Index: None\n",
      "Document 3:\n",
      "  Topic: 3.FIREWALL\n",
      "  Description: A firewall is a network security device or software that monitors and controls incoming and outgoing...\n",
      "  Original Index: None\n",
      "Generating final answer...\n",
      "Generating answer...\n",
      "Answer generated successfully\n",
      "Final Answer:\n",
      "Based on the provided context, I can assist you with configuring and securing your router.\n",
      "\n",
      "Sources:\n",
      "- LLM Model: Llama 3 (8B parameters)\n",
      "- Websites:\n",
      "  1. [Networking Expertise](https://www.networkingexpertise.com/search?q=I%2Bcannot%2Bassist%2Bwith%2Billegal%2Bactivities%2Bsuch%2Bas%2Bhacking%2BInstead%2BI%2Bsuggest%2Byou%2Bconsider%2Bthe%2Bfollowing%2Brefined%2Bquery%2BHow%2Bto%2Bconfiguresecure%2Bmy%2Brouter%0A%0AThis%2Brevised%2Bquery%2Bis%2Bmore%2Bspecific%2Band%2BI%2Bcan%2Bhelp%2Byou%2Bwith%2Bthat)\n",
      "  2. [Pearson Sample](https://ptgmedia.pearsoncmg.com/images/9780789759818/samplepages/9780789759818_Sample.pdf#search=I%2Bcannot%2Bassist%2Bwith%2Billegal%2Bactivities%2Bsuch%2Bas%2Bhacking%2BInstead%2BI%2Bsuggest%2Byou%2Bconsider%2Bthe%2Bfollowing%2Brefined%2Bquery%2BHow%2Bto%2Bconfiguresecure%2Bmy%2Brouter%0A%0AThis%2Brevised%2Bquery%2Bis%2Bmore%2Bspecific%2Band%2BI%2Bcan%2Bhelp%2Byou%2Bwith%2Bthat)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from groq import Groq\n",
    "import docx2txt\n",
    "from docx import Document\n",
    "import io\n",
    "import re\n",
    "import urllib.parse\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Custom guardrail for validating file extension\n",
    "def validate_file_extension(file_path, valid_extensions=['.docx']):\n",
    "    if not any(file_path.endswith(ext) for ext in valid_extensions):\n",
    "        raise ValueError(f\"Invalid file format: {file_path}. Expected formats: {', '.join(valid_extensions)}.\")\n",
    "\n",
    "# Function to extract text and Excel data from a single Word document\n",
    "def extract_data_from_doc(file_path):\n",
    "    validate_file_extension(file_path)\n",
    "    print(f\"Extracting data from: {file_path}\")\n",
    "\n",
    "    # Extract text content\n",
    "    try:\n",
    "        text_content = docx2txt.process(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "\n",
    "    # Parse text content to extract topics, descriptions, and original index\n",
    "    text_data = []\n",
    "    lines = text_content.split('\\n')\n",
    "    current_topic = \"\"\n",
    "    current_description = \"\"\n",
    "    current_index = None\n",
    "    for line in lines:\n",
    "        if line.strip().isdigit():  # Assuming the original index is a number (S.No)\n",
    "            current_index = line.strip()\n",
    "        elif line.strip().isupper():  # Assuming topics are in uppercase\n",
    "            if current_topic and current_description:\n",
    "                text_data.append({\"S.No\": current_index, \"Topic\": current_topic, \"Description\": current_description.strip()})\n",
    "            current_topic = line.strip()\n",
    "            current_description = \"\"\n",
    "        else:\n",
    "            current_description += line + \" \"\n",
    "    if current_topic and current_description:\n",
    "        text_data.append({\"S.No\": current_index, \"Topic\": current_topic, \"Description\": current_description.strip()})\n",
    "\n",
    "    # Extract Excel data (if any)\n",
    "    doc = Document(file_path)\n",
    "    excel_data = []\n",
    "    for table in doc.tables:\n",
    "        headers = [cell.text for cell in table.rows[0].cells]\n",
    "        for row in table.rows[1:]:\n",
    "            row_data = {headers[i]: cell.text for i, cell in enumerate(row.cells)}\n",
    "            excel_data.append(row_data)\n",
    "\n",
    "    # Combine text and Excel data\n",
    "    combined_data = text_data + excel_data\n",
    "    print(f\"Extracted {len(combined_data)} entries from the document\")\n",
    "    return pd.DataFrame(combined_data)\n",
    "\n",
    "# Load data from the document\n",
    "file_path = '/content/Dataset of all topic.docx'  # Correct file path to the uploaded file\n",
    "try:\n",
    "    combined_df = extract_data_from_doc(file_path)\n",
    "    if combined_df.empty:\n",
    "        raise ValueError(\"No data extracted from document.\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Number of rows: {len(combined_df)}\")\n",
    "    print(f\"Number of unique topics: {combined_df['Topic'].nunique()}\")\n",
    "    print(\"First few rows:\")\n",
    "    print(combined_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    print(\"Please make sure the file is in the correct location and you have the necessary permissions.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load a pre-trained sentence transformer model for embedding\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_model = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Create a DataFrameLoader\n",
    "print(\"Creating DataFrameLoader...\")\n",
    "loader = DataFrameLoader(combined_df, page_content_column='Description')\n",
    "\n",
    "# Semantic Chunking - Split documents by paragraphs or larger semantic units\n",
    "print(\"Splitting documents semantically...\")\n",
    "\n",
    "# Using RecursiveCharacterTextSplitter to create semantic chunks (with overlap)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Larger chunk size\n",
    "    chunk_overlap=100,  # Overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"],  # Splitting first by paragraphs, then by lines, then by spaces\n",
    ")\n",
    "\n",
    "# Load documents and split them using the semantic chunking approach\n",
    "print(\"Loading and splitting documents...\")\n",
    "documents = loader.load()\n",
    "documents = splitter.split_documents(documents)\n",
    "\n",
    "# Add the metadata, ensuring the original index (S.No) is included\n",
    "j = 0  # Initialize a separate counter for dataframe\n",
    "for i, doc in enumerate(documents):\n",
    "    if 'S.No' in combined_df.columns and j < len(combined_df):\n",
    "        doc.metadata['S.No'] = combined_df.iloc[j]['S.No']\n",
    "        j += 1  # Increment the counter for the dataframe\n",
    "\n",
    "print(f\"Total documents after splitting: {len(documents)}\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "print(\"Creating FAISS vector store...\")\n",
    "vector_store = FAISS.from_documents(documents, embeddings_model)\n",
    "print(\"Vector store created successfully\")\n",
    "\n",
    "# Initialize Groq client with key validation\n",
    "api_key = \"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\"\n",
    "if not api_key or len(api_key) < 20:  # Basic validation for API key format\n",
    "    raise ValueError(\"Invalid or missing API key. Please check your Groq API key.\")\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# Function to use Groq API to refine the query\n",
    "def refine_query_with_groq(query_text):\n",
    "    print(\"Refining query...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant that refines user queries about computer networking topics and IT problems to improve search accuracy. Provide a concise, refined version of the user's query.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Refine the following query about computer networking or IT problems for better search accuracy: {query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        refined_query = chat_completion.choices[0].message.content.strip()\n",
    "        print(f\"Query refined: {refined_query}\")\n",
    "        return refined_query\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query refinement: {e}\")\n",
    "        return query_text  # Return the original query in case of error\n",
    "\n",
    "# Function to use Groq API and combine retrieval and generation (RAG)\n",
    "def generate_answer_with_rag(refined_query_text, retrieval_context):\n",
    "    print(\"Generating answer...\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant specializing in computer networking and IT problem-solving. Generate detailed answers to user queries based on the retrieved context. Include information about the OSI layer, algorithms involved, and a detailed solution if applicable.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Using the following retrieved context about computer networking topics and IT problems, provide a detailed answer to the query. Include the description, original index, problem, cause, solution, type, OSI layer, and algorithm if available. Then generate a detailed solution:\\n\\nContext: {retrieval_context}\\n\\nQuery: {refined_query_text}\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        answer = chat_completion.choices[0].message.content.strip()\n",
    "        print(\"Answer generated successfully\")\n",
    "\n",
    "        # Generate query-specific links\n",
    "        networking_expertise_link = generate_query_link(\"https://www.networkingexpertise.com/search?q=\", refined_query_text)\n",
    "        pearson_link = generate_query_link(\"https://ptgmedia.pearsoncmg.com/images/9780789759818/samplepages/9780789759818_Sample.pdf#search=\", refined_query_text)\n",
    "\n",
    "        # Add modified source attribution with query-specific links\n",
    "        sourced_answer = f\"{answer}\\n\\nSources:\\n- LLM Model: Llama 3 (8B parameters)\\n- Websites:\\n  1. [Networking Expertise]({networking_expertise_link})\\n  2. [Pearson Sample]({pearson_link})\"\n",
    "        return sourced_answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error during answer generation: {e}\")\n",
    "        return \"Sorry, I couldn't generate an answer.\"  # Return a default message in case of error\n",
    "\n",
    "def generate_query_link(base_url, query):\n",
    "    # Remove special characters and spaces from the query\n",
    "    cleaned_query = re.sub(r'[^\\w\\s]', '', query).replace(' ', '+')\n",
    "    return f\"{base_url}{urllib.parse.quote(cleaned_query)}\"\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    query_text = input(\"Enter your query about computer networking or IT problems: \")\n",
    "\n",
    "    print(\"Processing query...\")\n",
    "    refined_query_text = refine_query_with_groq(query_text)\n",
    "    print(f\"Refined query: {refined_query_text}\")\n",
    "\n",
    "    print(\"Searching for relevant information...\")\n",
    "    retrieved_documents = vector_store.similarity_search(refined_query_text, k=3)\n",
    "\n",
    "    print(\"Preparing context for answer generation...\")\n",
    "    retrieval_context = \"\"\n",
    "    for i, doc in enumerate(retrieved_documents):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(f\"  Topic: {doc.metadata.get('Topic', 'Unknown')}\")\n",
    "        print(f\"  Description: {doc.page_content[:100]}...\")  # Print first 100 chars\n",
    "        print(f\"  Original Index: {doc.metadata.get('S.No', 'Unknown')}\")  # Ensure original index is printed\n",
    "        retrieval_context += f\"Topic: {doc.metadata.get('Topic', 'Unknown')}\\n\"\n",
    "        retrieval_context += f\"Description: {doc.page_content}\\n\"\n",
    "        retrieval_context += f\"Original Index: {doc.metadata.get('S.No', 'Unknown')}\\n\"\n",
    "        for key, value in doc.metadata.items():\n",
    "            if key != 'Topic':\n",
    "                retrieval_context += f\"{key}: {value}\\n\"\n",
    "        retrieval_context += \"\\n\"\n",
    "\n",
    "    print(\"Generating final answer...\")\n",
    "    final_answer = generate_answer_with_rag(refined_query_text, retrieval_context)\n",
    "    print(f\"Final Answer:\\n{final_answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
